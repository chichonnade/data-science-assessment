{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a8e573",
   "metadata": {},
   "source": [
    "# Data Engineering Take-Home Assignment: Nature Conservation & Geospatial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80660d",
   "metadata": {},
   "source": [
    "## Context\n",
    "Assume you have been hired as a Data Engineer for an organization focused on nature conservation. The organization is working on a project to monitor and protect natural habitats using satellite data, wildlife sensor data, and geospatial information. Your task is to design and implement a data pipeline that ingests, processes, and analyzes this data to help identify areas needing immediate conservation attention as well as build a model that provides helpful insights related our organization's interests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f8e14",
   "metadata": {},
   "source": [
    "## Objective \n",
    "\n",
    "Your goal in this assessment is to showcase your curiousity and creativity to design rigorous models and derive interesting insights.  \n",
    "\n",
    "You'll be given two tasks.\n",
    "\n",
    "The first is a design task, in which we expect you to diagram and describe how you'd set up a process to injest this data from a live streamed source, assuming you are also paying montoring services to supply this data from scratch. Think about how you might transform and store the data efficiently for querying and analysis and feed it into your model. \n",
    "\n",
    "The second task will require you devise interesting questions from preliminary explorations of a subset of migration data, found alongside this notebook, and construct a rigorous model to answer them. Please demonstrate all of your process using this notebook, and most importantly your outputs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae6885",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### 1) Design - Data Ingestion & Storage:\n",
    "- **Ingestion**: Design and implement a solution to ingest data from three different sources: GeoJSON, CSV, and JSON.\n",
    "- **Automation**: Ensure the pipeline can handle regular data updates (e.g., daily or hourly).\n",
    "- **Storage**: Choose appropriate storage solutions for each dataset (e.g., relational database, NoSQL, cloud storage, or data lake). Provide justification for your choices.\n",
    "\n",
    "### 2) Data Transformation & Analysis:\n",
    "- **Data Parsing & Cleaning**: \n",
    "  - Parse and clean the wildlife tracking data (CSV) and geospatial data (GeoJSON) to ensure consistency.\n",
    "  - Ensure the data is ready for analysis by standardizing formats, removing errors, and handling missing values.\n",
    "\n",
    "- **Exploratory Data Analysis**:\n",
    "  - Investigate the data to understand key characteristics, distributions, and trends.\n",
    "\n",
    "- **Behavioral Analysis**:\n",
    "  - Identify more complex animal behaviors:\n",
    "    - Determine when animals cross the boundaries of protected areas.\n",
    "    - Analyze potential factors contributing to these crossings (e.g., time, weather, or environmental changes).\n",
    "    - Calculate the total number of animal entries and exits from protected areas over time.\n",
    "\n",
    "- **Advanced Insights**:\n",
    "  - Identify migration paths or clustering patterns.\n",
    "  - Build a predictive model to anticipate future animal movements or identify risk zones for endangered species.\n",
    "\n",
    "### 3) Optional Bonus - Visualization/Reporting:\n",
    "- Provide interactive visualizations to demonstrate your analysis, ideally within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aefbcb",
   "metadata": {},
   "source": [
    "### Here are data sources you can use to build your analysis. \n",
    "\n",
    "- https://storage.googleapis.com/data-science-assessment/animal_events.csv\n",
    "- https://storage.googleapis.com/data-science-assessment/animals.csv\n",
    "- https://storage.googleapis.com/data-science-assessment/protected_areas.json\n",
    "- https://storage.googleapis.com/data-science-assessment/satellites.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ead913",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "#### Design component:\n",
    "- A clear description and diagrams for the architecture and tools you might used, including any cloud services, databases, or libraries (if applicable). During the discussion we'll go over different scenarios. \n",
    "\n",
    "#### Implementation:\n",
    "- Code for the data pipeline that includes:\n",
    "  - Data ingestion scripts or setup.\n",
    "  - Transformation and processing logic.\n",
    "  - Queries or outputs showcasing the results.\n",
    "- (Optional) a visualization of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8713c0",
   "metadata": {},
   "source": [
    "## Data\n",
    "### 1. **Animal Events - CSV** [Download link](https://storage.googleapis.com/data-science-assessment/animal_events.csv)\n",
    "\n",
    "- Contains data on animal movement events with details like location and speed.\n",
    "- **Key Columns**: `event_id`, `animal_id`, `timestamp`, `latitude`, `longitude`, `speed`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Animals - CSV** [Download link](https://storage.googleapis.com/data-science-assessment/animals.csv)\n",
    "\n",
    "- Metadata about tracked animals, including species and conservation status.\n",
    "- **Key Columns**: `animal_id`, `species`, `endangered`, `animal_type`, `preferred_landcover`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Protected Areas - GeoJSON** [Download link](https://storage.googleapis.com/data-science-assessment/protected_areas.json)\n",
    "\n",
    "- Geospatial data representing protected areas with boundaries and metadata.\n",
    "- **Key Fields**: `name`, `category`, `protected_area_id`, `geometry`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Satellite Metadata - JSON** [Download link](https://storage.googleapis.com/data-science-assessment/satellites.json)\n",
    "\n",
    "- Metadata from satellite imagery, covering factors like cloud cover and resolution.\n",
    "- **Key Fields**: `satellite_id`, `start_time`, `last_time`, `frequency`, `bounding_box`, `cloud_cover_percentage`, `resolution`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b856217-c9be-4789-a0f1-75f7c65819e5",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "- **Data Engineering Skills**: How well the pipeline handles ingestion, transformation, and storage.\n",
    "- **Geospatial Data Handling**: Ability to process geospatial data and perform spatial operations (e.g., joins, intersections).\n",
    "- **Scalability & Efficiency**: The pipelineâ€™s ability to handle larger datasets or more frequent updates.\n",
    "- **Code Quality**: Structure, readability, and use of best practices.\n",
    "- **Documentation**: Clear explanations of your approach and any assumptions made.\n",
    "- **Bonus (Visualization/Reporting)**: Extra points for insightful data visualization or reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb4ebc",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42307a26",
   "metadata": {},
   "source": [
    "Feel free to set up this notebook using condo, or your own kernal / virtual environment. To make it easier, you can set up the notebook using this docker with the potentialy libraries you might need. \n",
    "\n",
    "#### To start using a prepared Docker image, \n",
    "- 1 navigate to this shared folder in your terminal, and then load up docker and run the docker file to pull in needed libraries\n",
    "\n",
    "```bash\n",
    "docker build -t geospatial-notebook .\n",
    "docker run -p 8888:8888 -v $(pwd):/home/jovyan/work geospatial-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a081b",
   "metadata": {},
   "source": [
    "When the container runs, it will display a URL with a token (something like http://127.0.0.1:8888/?token=...). It will probably be something like http://127.0.0.1:8888/tree You can copy this URL into your browser, and you'll open to a Jupyter lab. Your existing notebook will be available inside the container under the work directory.\n",
    "\n",
    "Anytime you want to work again, just run the following command to start the Docker container and access your notebooks:\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 -v $(pwd):/home/jovyan/work geospatial-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c40610ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries you may or may not need\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import osgeo.gdal\n",
    "#extra\n",
    "import geoalchemy2\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5dae8",
   "metadata": {},
   "source": [
    "### Task 1: Data Ingestion & Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ac0ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using a postgres database to store our data from multiple sources\n",
    "# Prerequesites: \n",
    "# 1. Postgres 16 with the PostGIS extension installed on your machine\n",
    "# 2. Create a local database called 'assignment' accessible on port '5433' with username 'postgres' and password 'postgres'\n",
    "\n",
    "# Connect to your database\n",
    "db_username = 'postgres'  \n",
    "db_password = 'postgres'\n",
    "db_host = 'localhost'\n",
    "db_port = '5433'\n",
    "db_name = 'assignment'\n",
    "\n",
    "conn_string = f'postgresql://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}' # Create connection string\n",
    "engine = sqlalchemy.create_engine(conn_string) # Create database engine using sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22bf9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each data source, we define a function to ingest the data into the database\n",
    "\n",
    "# Ingest animal_events.csv\n",
    "def ingest_animal_events(csv_file):\n",
    "    animal_events = pd.read_csv(csv_file) # Read CSV data\n",
    "    animal_events['timestamp'] = pd.to_datetime(animal_events['timestamp']) # ensure proper datetime format\n",
    "    animal_events.to_sql('animal_events', engine, if_exists='replace', index=False) # Store in PostgreSQL\n",
    "\n",
    "# Ingest animals.csv\n",
    "def ingest_animals(csv_file):\n",
    "    animals = pd.read_csv(csv_file) # Read CSV data\n",
    "    animals.drop_duplicates(subset='animal_id', inplace=True) # Clean and transform data\n",
    "    animals.to_sql('animals', engine, if_exists='replace', index=False) # Store in PostgreSQL\n",
    "\n",
    "# Ingest ingest_protected_areas\n",
    "def ingest_protected_areas(geojson_file):\n",
    "    protected_areas = gpd.read_file(geojson_file) # Read GeoJSON data\n",
    "    protected_areas = protected_areas.to_crs(epsg=4326) # Ensure consistent Coordinate Reference System (crs)\n",
    "    protected_areas.to_postgis('protected_areas', engine, if_exists='replace', index=False) # Store in PostGIS\n",
    "\n",
    "# Ingest ingest_satellites\n",
    "def ingest_satellites(json_file):\n",
    "    with open(json_file) as f: # Read JSON data\n",
    "        satellites_data = json.load(f)\n",
    "    satellites_df = pd.json_normalize(satellites_data) # Convert JSON to DataFrame\n",
    "    satellites_df.to_sql('satellites', engine, if_exists='replace', index=False) # Store in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "061b79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the data ingestion\n",
    "ingest_animal_events('data/animal_events.csv')\n",
    "ingest_animals('data/animals.csv')\n",
    "ingest_protected_areas('data/protected_areas.json')\n",
    "ingest_satellites('data/satellites.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252f446",
   "metadata": {},
   "source": [
    "### Task 2: Data Transformation & Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e59b8",
   "metadata": {},
   "source": [
    "#### 2.1 Parse and Clean the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### animal_events ####\n",
    "animal_events = pd.read_sql('SELECT * FROM animal_events', engine) # Load data\n",
    "animal_events['timestamp'] = pd.to_datetime(animal_events['timestamp']) # Convert timestamp to datetime\n",
    "animal_events.drop_duplicates(subset=['event_id'], inplace=True) # Drop duplicates and handle missing values\n",
    "animal_events.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "geometry = [shapely.geometry.Point(xy) for xy in zip(animal_events['longitude'], animal_events['latitude'])] # Convert to a GeoDataFrame\n",
    "animal_events_gdf = gpd.GeoDataFrame(animal_events, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "#### protected_areas ####\n",
    "protected_areas_gdf = gpd.read_postgis('SELECT * FROM protected_areas', engine, geom_col='geometry') # Load data\n",
    "protected_areas_gdf = protected_areas_gdf.to_crs(epsg=4326) # Ensure correct CRS\n",
    "\n",
    "#### animals ####\n",
    "animals_df = pd.read_sql('SELECT * FROM animals', engine) # Load data\n",
    "animals_df.drop_duplicates(subset=['animal_id'], inplace=True) # Drop duplicates and handle missing values\n",
    "\n",
    "#### satellites ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522912f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a90a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
